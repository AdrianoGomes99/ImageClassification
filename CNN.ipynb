{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import needed packages and clone the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print('You are using numpy version:', np.__version__)\n",
    "\n",
    "!git clone https://github.com/Horea94/Fruit-Images-Dataset\n",
    "\n",
    "train_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/'\n",
    "test_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Test/'\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np.array(data['target'])\n",
    "    target_labels = np.array(data['target_names'])\n",
    "    return files, targets, target_labels\n",
    "\n",
    "names_train, intclass_train, stringclass_train = load_dataset(train_dir)\n",
    "names_test, intclass_test, stringclass_test = load_dataset(test_dir)\n",
    "\n",
    "n_classes = len(np.unique(intclass_test))\n",
    "\n",
    "train_hot_class = np_utils.to_categorical(intclass_train, n_classes)\n",
    "test_hot_class = np_utils.to_categorical(intclass_test, n_classes)\n",
    "\n",
    "def convert_image_to_array(files):\n",
    "    images_as_array=[]\n",
    "    for file in files:\n",
    "        # Convert to Numpy Array\n",
    "        images_as_array.append(img_to_array(load_img(file)))\n",
    "    return images_as_array\n",
    "\n",
    "train_images_array = np.array(convert_image_to_array(names_train))\n",
    "test_images_array = np.array(convert_image_to_array(names_test))\n",
    "\n",
    "train_images_array = train_images_array.astype('float32')/255\n",
    "test_images_array = test_images_array.astype('float32')/255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split training into another training and test (to reduce computation resources/time)\n",
    "train_images_array, new_test_images_array, train_hot_class, new_test_hot_class = train_test_split(train_images_array, train_hot_class, test_size = 0.3, random_state = 42)\n",
    "\n",
    "train_images_array, valid_images_array, train_hot_class, valid_hot_class = train_test_split(train_images_array, train_hot_class, test_size = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_hot_class = np_utils.to_categorical(intclass_train, n_classes)\n",
    "test_hot_class = np_utils.to_categorical(intclass_test, n_classes)\n",
    "\n",
    "train_images_array = np.array(convert_image_to_array(names_train))\n",
    "test_images_array = np.array(convert_image_to_array(names_test))\n",
    "\n",
    "train_images_array = train_images_array.astype('float32')/255\n",
    "test_images_array = test_images_array.astype('float32')/255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split training into another training and test (to reduce computation resources/time)\n",
    "train_images_array, new_test_images_array, train_hot_class, new_test_hot_class = train_test_split(train_images_array, train_hot_class, test_size = 0.3, random_state = 42)\n",
    "\n",
    "train_images_array, valid_images_array, train_hot_class, valid_hot_class = train_test_split(train_images_array, train_hot_class, test_size = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    model.add(Dense(50)) #900\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "print('Model built.')\n",
    "modelname = 'ann1'\n",
    "checkpointer = ModelCheckpoint(filepath = modelname+'.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "history = network.fit(train_images_array, train_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 100, #15\n",
    "    validation_data = (valid_images_array, valid_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer],\n",
    "    shuffle = True)\n",
    "\n",
    "network.save('model_'+modelname+'.h5')\n",
    "\n",
    "train_loss, train_acc = network.evaluate(train_images_array, train_hot_class, verbose=0)\n",
    "print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "valid_loss, valid_acc = network.evaluate(valid_images_array, valid_hot_class, verbose=0)\n",
    "print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))\n",
    "new_test_loss, new_test_acc = network.evaluate(new_test_images_array, new_test_hot_class, verbose=0)\n",
    "print('Restored model, new_test accuracy: {:5.2f}%'.format(100 * new_test_acc))\n",
    "test_loss, test_acc = network.evaluate(test_images_array, test_hot_class, verbose=0)\n",
    "print('Restored model, test accuracy: {:5.2f}%'.format(100 * test_acc))\n",
    "np.save(modelname+'_history.npy',history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "history2=np.load('my_history.npy',allow_pickle='TRUE').item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history2['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('model_ann1.h5')\n",
    "new_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 5))\n",
    "plt.plot(history.history['accuracy'], color = 'r')\n",
    "plt.plot(history.history['val_accuracy'], color = 'b')\n",
    "plt.title('Model Accuracy', weight = 'bold', fontsize = 16)\n",
    "plt.ylabel('accuracy', weight = 'bold', fontsize = 14)\n",
    "plt.xlabel('epoch', weight = 'bold', fontsize = 14)\n",
    "plt.ylim(0.6, 1.1)\n",
    "plt.xticks(weight = 'bold', fontsize = 12)\n",
    "plt.yticks(weight = 'bold', fontsize = 12)\n",
    "plt.legend(['train', 'val'], loc = 'lower right', prop = {'size': 14})\n",
    "plt.grid(color = 'y', linewidth = '0.5')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(1)  \n",
    "   \n",
    " # summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "   \n",
    " # summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}