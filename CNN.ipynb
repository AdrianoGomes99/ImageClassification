{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import needed packages and clone the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using numpy version: 1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Fruit-Images-Dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print('You are using numpy version:', np.__version__)\n",
    "\n",
    "!git clone https://github.com/Horea94/Fruit-Images-Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "#print(sklearn.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the path directory to the cloned test and train data.\n",
    "load_dataset() returns the filenames, integer classes and string classes that are stored in file directory.\n",
    "\n",
    "\n",
    "1.   names_train is a vector that contains the filepath of all images from the training set\n",
    "2.   names_test is a vector that contains the filepath of all images from the test set\n",
    "3.   intclass_train is a vector containing the int class values (1-131) of all images from the training set\n",
    "4.   intclass_test is a vector containing the int class values (1-131) of all images from the test set\n",
    "5.   stringclass_train is a vector containing the string label of class of all images from the training set\n",
    "6.   stringclass_test is a vector containing the string label of class of all images from the test set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete!\n",
      "Training set size :  67692\n",
      "Testing set size :  22688\n",
      "['Apple Braeburn' 'Apple Crimson Snow' 'Apple Golden 1' 'Apple Golden 2'\n",
      " 'Apple Golden 3' 'Apple Granny Smith' 'Apple Pink Lady' 'Apple Red 1'\n",
      " 'Apple Red 2' 'Apple Red 3' 'Apple Red Delicious' 'Apple Red Yellow 1'\n",
      " 'Apple Red Yellow 2' 'Apricot' 'Avocado' 'Avocado ripe' 'Banana'\n",
      " 'Banana Lady Finger' 'Banana Red' 'Beetroot' 'Blueberry' 'Cactus fruit'\n",
      " 'Cantaloupe 1' 'Cantaloupe 2' 'Carambula' 'Cauliflower' 'Cherry 1'\n",
      " 'Cherry 2' 'Cherry Rainier' 'Cherry Wax Black' 'Cherry Wax Red'\n",
      " 'Cherry Wax Yellow' 'Chestnut' 'Clementine' 'Cocos' 'Corn' 'Corn Husk'\n",
      " 'Cucumber Ripe' 'Cucumber Ripe 2' 'Dates' 'Eggplant' 'Fig' 'Ginger Root'\n",
      " 'Granadilla' 'Grape Blue' 'Grape Pink' 'Grape White' 'Grape White 2'\n",
      " 'Grape White 3' 'Grape White 4' 'Grapefruit Pink' 'Grapefruit White'\n",
      " 'Guava' 'Hazelnut' 'Huckleberry' 'Kaki' 'Kiwi' 'Kohlrabi' 'Kumquats'\n",
      " 'Lemon' 'Lemon Meyer' 'Limes' 'Lychee' 'Mandarine' 'Mango' 'Mango Red'\n",
      " 'Mangostan' 'Maracuja' 'Melon Piel de Sapo' 'Mulberry' 'Nectarine'\n",
      " 'Nectarine Flat' 'Nut Forest' 'Nut Pecan' 'Onion Red' 'Onion Red Peeled'\n",
      " 'Onion White' 'Orange' 'Papaya' 'Passion Fruit' 'Peach' 'Peach 2'\n",
      " 'Peach Flat' 'Pear' 'Pear 2' 'Pear Abate' 'Pear Forelle' 'Pear Kaiser'\n",
      " 'Pear Monster' 'Pear Red' 'Pear Stone' 'Pear Williams' 'Pepino'\n",
      " 'Pepper Green' 'Pepper Orange' 'Pepper Red' 'Pepper Yellow' 'Physalis'\n",
      " 'Physalis with Husk' 'Pineapple' 'Pineapple Mini' 'Pitahaya Red' 'Plum'\n",
      " 'Plum 2' 'Plum 3' 'Pomegranate' 'Pomelo Sweetie' 'Potato Red'\n",
      " 'Potato Red Washed' 'Potato Sweet' 'Potato White' 'Quince' 'Rambutan'\n",
      " 'Raspberry' 'Redcurrant' 'Salak' 'Strawberry' 'Strawberry Wedge'\n",
      " 'Tamarillo' 'Tangelo' 'Tomato 1' 'Tomato 2' 'Tomato 3' 'Tomato 4'\n",
      " 'Tomato Cherry Red' 'Tomato Heart' 'Tomato Maroon' 'Tomato Yellow'\n",
      " 'Tomato not Ripened' 'Walnut' 'Watermelon']\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/'\n",
    "test_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Test/'\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np.array(data['target'])\n",
    "    target_labels = np.array(data['target_names'])\n",
    "    return files, targets, target_labels\n",
    "    \n",
    "names_train, intclass_train, stringclass_train = load_dataset(train_dir)\n",
    "names_test, intclass_test, stringclass_test = load_dataset(test_dir)\n",
    "\n",
    "print('Loading complete!')\n",
    "print('Training set size : ',  names_train.shape[0])\n",
    "print('Testing set size : ', names_test.shape[0])\n",
    "print(stringclass_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show distribution of images to the different classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(intclass, counts) = np.unique(intclass_train, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "plt.title('distribution of full training and test data')\n",
    "full_train_frequencies = np.asarray((intclass, counts)).T\n",
    "\n",
    "(intclass, counts) = np.unique(intclass_test, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "full_test_frequencies = np.asarray((intclass, counts)).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show example file paths and their class label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 3:\n",
    "  print('Path to image : ', names_train[i])\n",
    "  print('Class label of image: ', intclass_train[i])\n",
    "  i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size : 67692\n",
      "Testing set size : 22688\n",
      "Number of unique classes: 131\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(intclass_test))\n",
    "print('Training set size :',  names_train.shape[0])\n",
    "print('Testing set size :', names_test.shape[0])\n",
    "print('Number of unique classes:', n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Categorized matrix (0 or 1) for class of each image.\n",
    "\n",
    "\n",
    "1.   train_hot_class is a list containing (length of train images) arrays containing a list each (131 elements) where the i th element is 1, when the image is classified as intclass i (otherwise 0)\n",
    "2.   test_hot_class is a list containing (length of test images) arrays containing a list each (131 elements) where the i th element is 1, when the image is classified as intclass i (otherwise 0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hot_class = np_utils.to_categorical(intclass_train, n_classes)\n",
    "test_hot_class = np_utils.to_categorical(intclass_test, n_classes)\n",
    "train_hot_class[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change name of image to actual pixel array.\n",
    "The _images_array are the inputs (100x100 pixels with 3 color channels).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape :  (67692, 100, 100, 3)\n",
      "Test set shape :  (22688, 100, 100, 3)\n",
      "1st training image shape  (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "def convert_image_to_array(files):\n",
    "    images_as_array=[]\n",
    "    for file in files:\n",
    "        # Convert to Numpy Array\n",
    "        images_as_array.append(img_to_array(load_img(file)))\n",
    "    return images_as_array\n",
    "\n",
    "train_images_array = np.array(convert_image_to_array(names_train))\n",
    "print('Training set shape : ', train_images_array.shape)\n",
    "\n",
    "test_images_array = np.array(convert_image_to_array(names_test))\n",
    "print('Test set shape : ', test_images_array.shape)\n",
    "\n",
    "print('1st training image shape ',train_images_array[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pixel arrays of one image (100x100 pixels, 3 color channels)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('1st training image as array',test_images_array[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rescale pixel values from 0-255 range to 0-1.\n",
    "\n",
    "\n",
    "1.   train_images_array is an array containing the normalized pixel values of the train images.\n",
    "2.   test_images_array is an array containing the normalized pixel values of the test images.\n",
    "3.   valid_images_array is an array containing the normalized pixel values of the validation images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_images_array = train_images_array.astype('float32')/255\n",
    "test_images_array = test_images_array.astype('float32')/255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the train datasets into a train dataset and a validation dataset (~75/25).\n",
    "\n",
    "\n",
    "1.   names_train is a list of names of the first half of the train dataset (actual training dataset)\n",
    "2.   names_valid is a list of names of the second half of the train dataset (validation dataset)\n",
    "3.   train_hot_class is the first half of train_hot_class (actual training categorial classes)\n",
    "4.   valid_hot_class is the second half of train_hot_class (validation categorial classes)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    o, m = divmod(len(a), n)\n",
    "    return (a[i*o+min(i, m):(i+1)*o+min(i+1, m)] for i in range(n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_CV_NeuralNetwork(x_data, y_data, k = 5):\n",
    "    rand = random.sample(range(x_data.shape[0]), x_data.shape[0])\n",
    "    k_split_rand = list(split(rand, k))\n",
    "    for i in range(k):\n",
    "        seq = k_split_rand[i]\n",
    "        mask = np.ones(x_data.shape[0],dtype=bool)\n",
    "        mask[seq]=0\n",
    "        #x_valid = x_data[seq]\n",
    "        #y_valid = y_data[seq]\n",
    "        #x_train = x_data[mask]\n",
    "        #y_train = y_data[mask]\n",
    "        network = build_NeuralNetwork()\n",
    "        print('Model built.')\n",
    "        #checkpointer = ModelCheckpoint(filepath = 'CVann1.hdf5', verbose = 0, save_best_only = True)\n",
    "\n",
    "        network.fit(x_data[mask], y_data[mask],\n",
    "            batch_size = 32, #32\n",
    "            epochs = 20, #15\n",
    "            validation_data = (x_data[seq], y_data[seq]),\n",
    "            verbose = 2,\n",
    "            #callbacks=[checkpointer],\n",
    "            shuffle = True)\n",
    "        network_name = 'model1CV'+str(i)+'.h5'\n",
    "        network.save(network_name)\n",
    "        train_loss, train_acc = network.evaluate(x_data[mask], y_data[mask], verbose=0)\n",
    "        print('Restored model, train accuracy: {:5.2f}%'.format(100 * train_acc))\n",
    "        valid_loss, valid_acc = network.evaluate(x_data[seq], y_data[seq], verbose=0)\n",
    "        print('Restored model, valid accuracy: {:5.2f}%'.format(100 * valid_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def build_NeuralNetwork():\n",
    "\n",
    "    #input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(100,100,3)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #hiddenlayer 1\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    #hiddenlayer 2\n",
    "    #model.add(Dense(300)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 3\n",
    "    #model.add(Dense(1000)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    #hiddenlayer 4\n",
    "    #model.add(Dense(1000)) #900\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    print('Compiled!')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled!\n",
      "Model built.\n",
      "Epoch 1/20\n",
      "1058/1058 - 22s - loss: 4.7038 - accuracy: 0.0232 - val_loss: 4.5643 - val_accuracy: 0.0264 - 22s/epoch - 21ms/step\n",
      "Epoch 2/20\n",
      "1058/1058 - 5s - loss: 4.4995 - accuracy: 0.0281 - val_loss: 4.4464 - val_accuracy: 0.0358 - 5s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1058/1058 - 4s - loss: 4.3990 - accuracy: 0.0341 - val_loss: 4.7287 - val_accuracy: 0.0108 - 4s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1058/1058 - 4s - loss: 4.3321 - accuracy: 0.0361 - val_loss: 4.4000 - val_accuracy: 0.0324 - 4s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1058/1058 - 4s - loss: 4.2791 - accuracy: 0.0355 - val_loss: 4.7444 - val_accuracy: 0.0193 - 4s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1058/1058 - 4s - loss: 4.2380 - accuracy: 0.0377 - val_loss: 4.8672 - val_accuracy: 0.0139 - 4s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1058/1058 - 4s - loss: 4.2101 - accuracy: 0.0395 - val_loss: 4.4833 - val_accuracy: 0.0272 - 4s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1058/1058 - 4s - loss: 4.1810 - accuracy: 0.0386 - val_loss: 4.1429 - val_accuracy: 0.0471 - 4s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1058/1058 - 4s - loss: 4.1621 - accuracy: 0.0420 - val_loss: 4.1794 - val_accuracy: 0.0328 - 4s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1058/1058 - 4s - loss: 4.1446 - accuracy: 0.0409 - val_loss: 4.0985 - val_accuracy: 0.0401 - 4s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1058/1058 - 4s - loss: 4.1209 - accuracy: 0.0422 - val_loss: 4.0910 - val_accuracy: 0.0469 - 4s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1058/1058 - 4s - loss: 4.1152 - accuracy: 0.0421 - val_loss: 4.0823 - val_accuracy: 0.0337 - 4s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1058/1058 - 4s - loss: 4.0937 - accuracy: 0.0440 - val_loss: 4.2068 - val_accuracy: 0.0264 - 4s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1058/1058 - 4s - loss: 4.0885 - accuracy: 0.0428 - val_loss: 4.0836 - val_accuracy: 0.0412 - 4s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1058/1058 - 4s - loss: 4.0708 - accuracy: 0.0458 - val_loss: 4.0474 - val_accuracy: 0.0425 - 4s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1058/1058 - 4s - loss: 4.0626 - accuracy: 0.0436 - val_loss: 4.0186 - val_accuracy: 0.0453 - 4s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1058/1058 - 4s - loss: 4.0572 - accuracy: 0.0460 - val_loss: 4.0907 - val_accuracy: 0.0350 - 4s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1058/1058 - 4s - loss: 4.0460 - accuracy: 0.0464 - val_loss: 3.9922 - val_accuracy: 0.0482 - 4s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1058/1058 - 4s - loss: 4.0380 - accuracy: 0.0449 - val_loss: 3.9893 - val_accuracy: 0.0485 - 4s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1058/1058 - 4s - loss: 4.0258 - accuracy: 0.0449 - val_loss: 4.0065 - val_accuracy: 0.0437 - 4s/epoch - 4ms/step\n",
      "Restored model, train accuracy:  4.58%\n",
      "Restored model, valid accuracy:  4.37%\n",
      "Compiled!\n",
      "Model built.\n",
      "Epoch 1/20\n",
      "1058/1058 - 8s - loss: 4.6925 - accuracy: 0.0227 - val_loss: 4.5829 - val_accuracy: 0.0292 - 8s/epoch - 8ms/step\n",
      "Epoch 2/20\n",
      "1058/1058 - 4s - loss: 4.4761 - accuracy: 0.0271 - val_loss: 4.4266 - val_accuracy: 0.0358 - 4s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1058/1058 - 4s - loss: 4.3756 - accuracy: 0.0304 - val_loss: 4.3902 - val_accuracy: 0.0272 - 4s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "1058/1058 - 4s - loss: 4.3074 - accuracy: 0.0324 - val_loss: 4.2940 - val_accuracy: 0.0319 - 4s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1058/1058 - 4s - loss: 4.2572 - accuracy: 0.0335 - val_loss: 4.2008 - val_accuracy: 0.0367 - 4s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1058/1058 - 4s - loss: 4.2174 - accuracy: 0.0348 - val_loss: 4.1739 - val_accuracy: 0.0367 - 4s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1058/1058 - 4s - loss: 4.1865 - accuracy: 0.0352 - val_loss: 4.1518 - val_accuracy: 0.0363 - 4s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1058/1058 - 4s - loss: 4.1597 - accuracy: 0.0360 - val_loss: 4.2313 - val_accuracy: 0.0316 - 4s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1058/1058 - 4s - loss: 4.1380 - accuracy: 0.0378 - val_loss: 4.2565 - val_accuracy: 0.0309 - 4s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1058/1058 - 4s - loss: 4.1147 - accuracy: 0.0384 - val_loss: 4.0560 - val_accuracy: 0.0412 - 4s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1058/1058 - 4s - loss: 4.0995 - accuracy: 0.0385 - val_loss: 4.0780 - val_accuracy: 0.0398 - 4s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1058/1058 - 4s - loss: 4.0819 - accuracy: 0.0378 - val_loss: 4.3330 - val_accuracy: 0.0313 - 4s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1058/1058 - 4s - loss: 4.0684 - accuracy: 0.0386 - val_loss: 4.0052 - val_accuracy: 0.0428 - 4s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1058/1058 - 4s - loss: 4.0572 - accuracy: 0.0397 - val_loss: 3.9933 - val_accuracy: 0.0424 - 4s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1058/1058 - 4s - loss: 4.0446 - accuracy: 0.0401 - val_loss: 3.9921 - val_accuracy: 0.0452 - 4s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1058/1058 - 4s - loss: 4.0300 - accuracy: 0.0406 - val_loss: 4.1918 - val_accuracy: 0.0301 - 4s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1058/1058 - 4s - loss: 4.0195 - accuracy: 0.0404 - val_loss: 4.0886 - val_accuracy: 0.0403 - 4s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1058/1058 - 4s - loss: 4.0095 - accuracy: 0.0424 - val_loss: 4.0133 - val_accuracy: 0.0381 - 4s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1058/1058 - 4s - loss: 3.9968 - accuracy: 0.0420 - val_loss: 3.9936 - val_accuracy: 0.0383 - 4s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1058/1058 - 4s - loss: 3.9885 - accuracy: 0.0419 - val_loss: 3.9930 - val_accuracy: 0.0392 - 4s/epoch - 4ms/step\n",
      "Restored model, train accuracy:  3.68%\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.78 GiB for an array with shape (33846, 100, 100, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-a09af20b435c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcompute_CV_NeuralNetwork\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_images_array\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_hot_class\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-8-5e4feebd9d15>\u001B[0m in \u001B[0;36mcompute_CV_NeuralNetwork\u001B[1;34m(x_data, y_data, k)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mtrain_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Restored model, train accuracy: {:5.2f}%'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtrain_acc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[0mvalid_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mseq\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mseq\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Restored model, valid accuracy: {:5.2f}%'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mvalid_acc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.78 GiB for an array with shape (33846, 100, 100, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "compute_CV_NeuralNetwork(train_images_array, train_hot_class, k = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('Newann1CV1.h5')\n",
    "loss, acc = new_model.evaluate(test_images_array, test_hot_class, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images_array, valid1_images_array, train1_hot_class, valid1_hot_class = train_test_split(train_images_array, train_hot_class,    test_size = 0.1, random_state=12)\n",
    "print('Vaildation structure of pixels : ', valid1_images_array.shape)\n",
    "print('Vaildation structure of categorial classes :', valid1_hot_class.shape)\n",
    "print('Train structure of images: ', train1_images_array.shape)\n",
    "print('Train structure of categorial classes : ', train1_hot_class.shape)\n",
    "print('------')\n",
    "#print('Test structure of images: ', test_images_array.shape)\n",
    "#print('Test structure of categorial classes : ', test_hot_class.shape)\n",
    "#valid1_images_array, test1_images_array, valid1_hot_class, test1_hot_class = train_test_split(valid1_images_array, valid1_hot_class, test_size = 0.5, random_state=12)\n",
    "print('Train structure of images: ', train1_images_array.shape)\n",
    "print('Train structure of categorial classes : ', train1_hot_class.shape)\n",
    "print('Vaildation structure of pixels : ', valid1_images_array.shape)\n",
    "print('Vaildation structure of categorial classes :', valid1_hot_class.shape)\n",
    "print('Test structure of images: ', test1_images_array.shape)\n",
    "print('Test structure of categorial classes : ', test1_hot_class.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show 10 images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize =(30,5))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(train1_images_array[i]))\n",
    "    print(names_train[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize =(30,5))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(test_images_array[i]))\n",
    "    print(names_test[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "build neural network model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = build_NeuralNetwork()\n",
    "network.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath = 'ann3.hdf5', verbose = 2, save_best_only = True)\n",
    "#filename='firstANN.csv'\n",
    "#history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "history = network.fit(train1_images_array, train1_hot_class,\n",
    "    batch_size = 32, #32\n",
    "    epochs = 25, #15\n",
    "    validation_data = (valid1_images_array, valid1_hot_class),\n",
    "    verbose = 2,\n",
    "    callbacks=[checkpointer], shuffle = True)\n",
    "network.save('ann3.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('ann3.h5')\n",
    "loss, acc = new_model.evaluate(valid1_images_array, valid1_hot_class, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate(test_images_array, test_hot_class, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_weights('ann.hdf5')\n",
    "score = model.evaluate(test_images_array, test_hot_class, verbose = 0)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(train_images_array[10].reshape(1,100,100,1))\n",
    " \n",
    "def display_activation(activations, col_size, row_size, act_index): \n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history.model.weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(test_images_array)\n",
    "\n",
    "fig = plt.figure(figsize=(32, 30))\n",
    "for i, idx in enumerate(np.random.choice(test_images_array.shape[0], size = 32, replace = False)):\n",
    "    ax = fig.add_subplot(8, 6, i + 1, xticks = [], yticks = [])\n",
    "    ax.imshow(np.squeeze(test_images_array[idx]))\n",
    "    pred_idx = np.argmax(y_pred[idx])\n",
    "    true_idx = np.argmax(test_hot_class[idx])\n",
    "    ax.set_title(\"{} ({})\".format(stringclass_test[pred_idx], stringclass_test[true_idx]),\n",
    "                 color = (\"green\" if pred_idx == true_idx else \"red\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 5))\n",
    "plt.plot(history.history['accuracy'], color = 'r')\n",
    "plt.plot(history.history['val_accuracy'], color = 'b')\n",
    "plt.title('Model Accuracy', weight = 'bold', fontsize = 16)\n",
    "plt.ylabel('accuracy', weight = 'bold', fontsize = 14)\n",
    "plt.xlabel('epoch', weight = 'bold', fontsize = 14)\n",
    "plt.ylim(0.4, 1)\n",
    "plt.xticks(weight = 'bold', fontsize = 12)\n",
    "plt.yticks(weight = 'bold', fontsize = 12)\n",
    "plt.legend(['train', 'val'], loc = 'lower right', prop = {'size': 14})\n",
    "plt.grid(color = 'y', linewidth = '0.5')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(1)  \n",
    "   \n",
    " # summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "   \n",
    " # summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}