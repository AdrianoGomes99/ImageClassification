{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx_V32mA_BDB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import numpy and sklearn method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "print(np.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy dataset from git repo. TESTTest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Fruit-Images-Dataset'...\n",
      "Updating files:   1% (1728/90503)\n",
      "Updating files:   2% (1811/90503)\n",
      "Updating files:   3% (2716/90503)\n",
      "Updating files:   4% (3621/90503)\n",
      "Updating files:   4% (3833/90503)\n",
      "Updating files:   5% (4526/90503)\n",
      "Updating files:   6% (5431/90503)\n",
      "Updating files:   6% (5812/90503)\n",
      "Updating files:   7% (6336/90503)\n",
      "Updating files:   8% (7241/90503)\n",
      "Updating files:   8% (7536/90503)\n",
      "Updating files:   9% (8146/90503)\n",
      "Updating files:  10% (9051/90503)\n",
      "Updating files:  10% (9523/90503)\n",
      "Updating files:  11% (9956/90503)\n",
      "Updating files:  12% (10861/90503)\n",
      "Updating files:  12% (11196/90503)\n",
      "Updating files:  13% (11766/90503)\n",
      "Updating files:  14% (12671/90503)\n",
      "Updating files:  14% (13249/90503)\n",
      "Updating files:  15% (13576/90503)\n",
      "Updating files:  16% (14481/90503)\n",
      "Updating files:  16% (15051/90503)\n",
      "Updating files:  17% (15386/90503)\n",
      "Updating files:  18% (16291/90503)\n",
      "Updating files:  19% (17196/90503)\n",
      "Updating files:  19% (17200/90503)\n",
      "Updating files:  20% (18101/90503)\n",
      "Updating files:  21% (19006/90503)\n",
      "Updating files:  21% (19252/90503)\n",
      "Updating files:  22% (19911/90503)\n",
      "Updating files:  23% (20816/90503)\n",
      "Updating files:  23% (21371/90503)\n",
      "Updating files:  24% (21721/90503)\n",
      "Updating files:  25% (22626/90503)\n",
      "Updating files:  25% (23403/90503)\n",
      "Updating files:  26% (23531/90503)\n",
      "Updating files:  27% (24436/90503)\n",
      "Updating files:  27% (25057/90503)\n",
      "Updating files:  28% (25341/90503)\n",
      "Updating files:  29% (26246/90503)\n",
      "Updating files:  30% (27151/90503)\n",
      "Updating files:  30% (27182/90503)\n",
      "Updating files:  31% (28056/90503)\n",
      "Updating files:  32% (28961/90503)\n",
      "Updating files:  32% (29152/90503)\n",
      "Updating files:  33% (29866/90503)\n",
      "Updating files:  34% (30772/90503)\n",
      "Updating files:  34% (31272/90503)\n",
      "Updating files:  35% (31677/90503)\n",
      "Updating files:  36% (32582/90503)\n",
      "Updating files:  36% (33209/90503)\n",
      "Updating files:  37% (33487/90503)\n",
      "Updating files:  38% (34392/90503)\n",
      "Updating files:  38% (35160/90503)\n",
      "Updating files:  39% (35297/90503)\n",
      "Updating files:  40% (36202/90503)\n",
      "Updating files:  40% (37090/90503)\n",
      "Updating files:  41% (37107/90503)\n",
      "Updating files:  42% (38012/90503)\n",
      "Updating files:  42% (38846/90503)\n",
      "Updating files:  43% (38917/90503)\n",
      "Updating files:  44% (39822/90503)\n",
      "Updating files:  45% (40727/90503)\n",
      "Updating files:  45% (40862/90503)\n",
      "Updating files:  46% (41632/90503)\n",
      "Updating files:  47% (42537/90503)\n",
      "Updating files:  47% (42943/90503)\n",
      "Updating files:  48% (43442/90503)\n",
      "Updating files:  49% (44347/90503)\n",
      "Updating files:  49% (44895/90503)\n",
      "Updating files:  50% (45252/90503)\n",
      "Updating files:  51% (46157/90503)\n",
      "Updating files:  51% (47051/90503)\n",
      "Updating files:  52% (47062/90503)\n",
      "Updating files:  53% (47967/90503)\n",
      "Updating files:  53% (48761/90503)\n",
      "Updating files:  54% (48872/90503)\n",
      "Updating files:  55% (49777/90503)\n",
      "Updating files:  55% (50140/90503)\n",
      "Updating files:  56% (50682/90503)\n",
      "Updating files:  57% (51587/90503)\n",
      "Updating files:  57% (52237/90503)\n",
      "Updating files:  58% (52492/90503)\n",
      "Updating files:  59% (53397/90503)\n",
      "Updating files:  59% (54283/90503)\n",
      "Updating files:  60% (54302/90503)\n",
      "Updating files:  61% (55207/90503)\n",
      "Updating files:  62% (56112/90503)\n",
      "Updating files:  62% (56415/90503)\n",
      "Updating files:  63% (57017/90503)\n",
      "Updating files:  64% (57922/90503)\n",
      "Updating files:  64% (58388/90503)\n",
      "Updating files:  65% (58827/90503)\n",
      "Updating files:  66% (59732/90503)\n",
      "Updating files:  66% (60213/90503)\n",
      "Updating files:  67% (60638/90503)\n",
      "Updating files:  68% (61543/90503)\n",
      "Updating files:  68% (62062/90503)\n",
      "Updating files:  69% (62448/90503)\n",
      "Updating files:  70% (63353/90503)\n",
      "Updating files:  70% (63518/90503)\n",
      "Updating files:  71% (64258/90503)\n",
      "Updating files:  71% (65046/90503)\n",
      "Updating files:  72% (65163/90503)\n",
      "Updating files:  73% (66068/90503)\n",
      "Updating files:  73% (66567/90503)\n",
      "Updating files:  74% (66973/90503)\n",
      "Updating files:  75% (67878/90503)\n",
      "Updating files:  75% (68102/90503)\n",
      "Updating files:  76% (68783/90503)\n",
      "Updating files:  76% (69544/90503)\n",
      "Updating files:  77% (69688/90503)\n",
      "Updating files:  77% (70554/90503)\n",
      "Updating files:  78% (70593/90503)\n",
      "Updating files:  79% (71498/90503)\n",
      "Updating files:  79% (72134/90503)\n",
      "Updating files:  80% (72403/90503)\n",
      "Updating files:  81% (73308/90503)\n",
      "Updating files:  81% (73653/90503)\n",
      "Updating files:  82% (74213/90503)\n",
      "Updating files:  82% (75117/90503)\n",
      "Updating files:  83% (75118/90503)\n",
      "Updating files:  84% (76023/90503)\n",
      "Updating files:  84% (76627/90503)\n",
      "Updating files:  85% (76928/90503)\n",
      "Updating files:  86% (77833/90503)\n",
      "Updating files:  86% (78134/90503)\n",
      "Updating files:  87% (78738/90503)\n",
      "Updating files:  88% (79643/90503)\n",
      "Updating files:  88% (79803/90503)\n",
      "Updating files:  89% (80548/90503)\n",
      "Updating files:  89% (81150/90503)\n",
      "Updating files:  90% (81453/90503)\n",
      "Updating files:  91% (82358/90503)\n",
      "Updating files:  91% (82740/90503)\n",
      "Updating files:  92% (83263/90503)\n",
      "Updating files:  93% (84168/90503)\n",
      "Updating files:  93% (84326/90503)\n",
      "Updating files:  94% (85073/90503)\n",
      "Updating files:  94% (85859/90503)\n",
      "Updating files:  95% (85978/90503)\n",
      "Updating files:  96% (86883/90503)\n",
      "Updating files:  96% (87372/90503)\n",
      "Updating files:  97% (87788/90503)\n",
      "Updating files:  98% (88693/90503)\n",
      "Updating files:  98% (88857/90503)\n",
      "Updating files:  99% (89598/90503)\n",
      "Updating files:  99% (90402/90503)\n",
      "Updating files:  99% (90445/90503)\n",
      "Updating files:  99% (90494/90503)\n",
      "Updating files: 100% (90503/90503)\n",
      "Updating files: 100% (90503/90503), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Horea94/Fruit-Images-Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the path directory to the cloned test and train data.\n",
    "load_dataset() returns the filenames, integer classes and string classes that are stored in file directory.\n",
    "\n",
    "\n",
    "1.   names_train is a vector that contains the filepath of all images from the training set\n",
    "2.   names_test is a vector that contains the filepath of all images from the test set\n",
    "3.   intclass_train is a vector containing the int class values (1-131) of all images from the training set\n",
    "4.   intclass_test is a vector containing the int class values (1-131) of all images from the test set\n",
    "5.   stringclass_train is a vector containing the string label of class of all images from the training set\n",
    "6.   stringclass_test is a vector containing the string label of class of all images from the test set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete!\n",
      "Training set size :  67692\n",
      "Testing set size :  22688\n",
      "['Apple Braeburn' 'Apple Crimson Snow' 'Apple Golden 1' 'Apple Golden 2'\n",
      " 'Apple Golden 3' 'Apple Granny Smith' 'Apple Pink Lady' 'Apple Red 1'\n",
      " 'Apple Red 2' 'Apple Red 3' 'Apple Red Delicious' 'Apple Red Yellow 1'\n",
      " 'Apple Red Yellow 2' 'Apricot' 'Avocado' 'Avocado ripe' 'Banana'\n",
      " 'Banana Lady Finger' 'Banana Red' 'Beetroot' 'Blueberry' 'Cactus fruit'\n",
      " 'Cantaloupe 1' 'Cantaloupe 2' 'Carambula' 'Cauliflower' 'Cherry 1'\n",
      " 'Cherry 2' 'Cherry Rainier' 'Cherry Wax Black' 'Cherry Wax Red'\n",
      " 'Cherry Wax Yellow' 'Chestnut' 'Clementine' 'Cocos' 'Corn' 'Corn Husk'\n",
      " 'Cucumber Ripe' 'Cucumber Ripe 2' 'Dates' 'Eggplant' 'Fig' 'Ginger Root'\n",
      " 'Granadilla' 'Grape Blue' 'Grape Pink' 'Grape White' 'Grape White 2'\n",
      " 'Grape White 3' 'Grape White 4' 'Grapefruit Pink' 'Grapefruit White'\n",
      " 'Guava' 'Hazelnut' 'Huckleberry' 'Kaki' 'Kiwi' 'Kohlrabi' 'Kumquats'\n",
      " 'Lemon' 'Lemon Meyer' 'Limes' 'Lychee' 'Mandarine' 'Mango' 'Mango Red'\n",
      " 'Mangostan' 'Maracuja' 'Melon Piel de Sapo' 'Mulberry' 'Nectarine'\n",
      " 'Nectarine Flat' 'Nut Forest' 'Nut Pecan' 'Onion Red' 'Onion Red Peeled'\n",
      " 'Onion White' 'Orange' 'Papaya' 'Passion Fruit' 'Peach' 'Peach 2'\n",
      " 'Peach Flat' 'Pear' 'Pear 2' 'Pear Abate' 'Pear Forelle' 'Pear Kaiser'\n",
      " 'Pear Monster' 'Pear Red' 'Pear Stone' 'Pear Williams' 'Pepino'\n",
      " 'Pepper Green' 'Pepper Orange' 'Pepper Red' 'Pepper Yellow' 'Physalis'\n",
      " 'Physalis with Husk' 'Pineapple' 'Pineapple Mini' 'Pitahaya Red' 'Plum'\n",
      " 'Plum 2' 'Plum 3' 'Pomegranate' 'Pomelo Sweetie' 'Potato Red'\n",
      " 'Potato Red Washed' 'Potato Sweet' 'Potato White' 'Quince' 'Rambutan'\n",
      " 'Raspberry' 'Redcurrant' 'Salak' 'Strawberry' 'Strawberry Wedge'\n",
      " 'Tamarillo' 'Tangelo' 'Tomato 1' 'Tomato 2' 'Tomato 3' 'Tomato 4'\n",
      " 'Tomato Cherry Red' 'Tomato Heart' 'Tomato Maroon' 'Tomato Yellow'\n",
      " 'Tomato not Ripened' 'Walnut' 'Watermelon']\n"
     ]
    }
   ],
   "source": [
    "#train_dir_adriano = ''\n",
    "#test_dir = ''\n",
    "train_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/'\n",
    "test_dir = 'C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Test/'\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np.array(data['target'])\n",
    "    target_labels = np.array(data['target_names'])\n",
    "    return files, targets, target_labels\n",
    "    \n",
    "names_train, intclass_train, stringclass_train = load_dataset(train_dir)\n",
    "names_test, intclass_test, stringclass_test = load_dataset(test_dir)\n",
    "\n",
    "print('Loading complete!')\n",
    "print('Training set size : ',  names_train.shape[0])\n",
    "print('Testing set size : ', names_test.shape[0])\n",
    "print(stringclass_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show distribution of images to the different classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(intclass, counts) = np.unique(intclass_train, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "plt.title('distribution of full training and test data')\n",
    "full_train_frequencies = np.asarray((intclass, counts)).T\n",
    "\n",
    "(intclass, counts) = np.unique(intclass_test, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "full_test_frequencies = np.asarray((intclass, counts)).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Datasets can be reduced for compiling: original size of the training dataset is 67692 images; original size of the test dataset is 22688 images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_datasize = 40000\n",
    "#test_datasize = 10000\n",
    "#names_train = names_train[:train_datasize]\n",
    "#names_test = names_test[:test_datasize]\n",
    "#intclass_train = intclass_train[:train_datasize]\n",
    "#intclass_test = intclass_test[:test_datasize]\n",
    "#print('Training set size : ',  names_train.shape[0])\n",
    "#print('Testing set size : ', names_test.shape[0])\n",
    "#print(stringclass_train)\n",
    "#print(intclass_train.shape)\n",
    "#print(intclass_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(intclass, counts) = np.unique(intclass_train, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "plt.title('distribution of reduced training and test data')\n",
    "reduced_train_frequencies = np.asarray((intclass, counts)).T\n",
    "\n",
    "(intclass, counts) = np.unique(intclass_test, return_counts=True)\n",
    "plt.bar(intclass, counts)\n",
    "reduced_test_frequencies = np.asarray((intclass, counts)).T\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#difference_train = full_train_frequencies - reduced_train_frequencies\n",
    "#print(difference_train)\n",
    "#difference_test = full_test_frequencies - reduced_test_frequencies\n",
    "#print(difference_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Cherry 2\\r_263_100.jpg\n",
      "Intclass :  27\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Nut Pecan\\73_100.jpg\n",
      "Intclass :  73\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Melon Piel de Sapo\\r_45_100.jpg\n",
      "Intclass :  68\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Redcurrant\\120_100.jpg\n",
      "Intclass :  114\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Strawberry Wedge\\r_176_100.jpg\n",
      "Intclass :  117\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Peach\\206_100.jpg\n",
      "Intclass :  80\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Nut Forest\\87_100.jpg\n",
      "Intclass :  72\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Apple Golden 1\\255_100.jpg\n",
      "Intclass :  2\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Pear Stone\\r_279_100.jpg\n",
      "Intclass :  90\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Watermelon\\105_100.jpg\n",
      "Intclass :  130\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Lychee\\103_100.jpg\n",
      "Intclass :  62\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Cantaloupe 1\\r_294_100.jpg\n",
      "Intclass :  22\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Tomato 2\\57_100.jpg\n",
      "Intclass :  121\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Cherry Wax Yellow\\197_100.jpg\n",
      "Intclass :  31\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Tomato Yellow\\166_100.jpg\n",
      "Intclass :  127\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Apple Granny Smith\\300_100.jpg\n",
      "Intclass :  5\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Cherry Rainier\\r_84_100.jpg\n",
      "Intclass :  28\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Grape White\\r_121_100.jpg\n",
      "Intclass :  46\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Limes\\r_157_100.jpg\n",
      "Intclass :  61\n",
      "Name :  C:/Users/crysi/Private Dokumente/Papers & wissenschaftl. Arbeiten/Master/Foundations of Machine Learning/First Project/Pycharm/Fruit-Images-Dataset/Training/Tomato Cherry Red\\126_100.jpg\n",
      "Intclass :  124\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 20:\n",
    "  print('Name : ', names_train[i])\n",
    "  print('Intclass : ', intclass_train[i])\n",
    "  i+=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  67692\n",
      "Testing set size :  22688\n"
     ]
    }
   ],
   "source": [
    "print('Training set size : ',  names_train.shape[0])\n",
    "print('Testing set size : ', names_test.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Amount of different classes in the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "131"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(np.unique(intclass_test))\n",
    "n_classes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Categorized matrix (0 or 1) for class of each image.\n",
    "\n",
    "\n",
    "1.   train_hot_class is a list containing (length of train images) arrays containing a list each (131 elements) where the i th element is 1, when the image is classified as intclass i (otherwise 0)\n",
    "2.   test_hot_class is a list containing (length of test images) arrays containing a list each (131 elements) where the i th element is 1, when the image is classified as intclass i (otherwise 0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "train_hot_class = np_utils.to_categorical(intclass_train, n_classes)\n",
    "test_hot_class = np_utils.to_categorical(intclass_test, n_classes)\n",
    "train_hot_class[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change name of image to actual pixel array.\n",
    "The _images_array are the inputs (100x100 pixels with 3 color channels).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape :  (67692, 100, 100, 3)\n",
      "Test set shape :  (22688, 100, 100, 3)\n",
      "1st training image shape  (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.utils import array_to_img, img_to_array, load_img\n",
    "\n",
    "def convert_image_to_array(files):\n",
    "    images_as_array=[]\n",
    "    for file in files:\n",
    "        # Convert to Numpy Array\n",
    "        images_as_array.append(img_to_array(load_img(file)))\n",
    "    return images_as_array\n",
    "\n",
    "train_images_array = np.array(convert_image_to_array(names_train))\n",
    "print('Training set shape : ', train_images_array.shape)\n",
    "\n",
    "test_images_array = np.array(convert_image_to_array(names_test))\n",
    "print('Test set shape : ', test_images_array.shape)\n",
    "\n",
    "print('1st training image shape ',train_images_array[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pixel arrays of one image (100x100 pixels, 3 color channels)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('1st training image as array',train_images_array[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rescale pixel values from 0-255 range to 0-1.\n",
    "\n",
    "\n",
    "1.   train_images_array is an array containing the normalized pixel values of the train images.\n",
    "2.   test_images_array is an array containing the normalized pixel values of the test images.\n",
    "3.   valid_images_array is an array containing the normalized pixel values of the validation images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_images_array = train_images_array.astype('float32')/255\n",
    "test_images_array = test_images_array.astype('float32')/255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_hot_class[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the train datasets into a train dataset and a validation dataset (~75/25).\n",
    "\n",
    "\n",
    "1.   names_train is a list of names of the first half of the train dataset (actual training dataset)\n",
    "2.   names_valid is a list of names of the second half of the train dataset (validation dataset)\n",
    "3.   train_hot_class is the first half of train_hot_class (actual training categorial classes)\n",
    "4.   valid_hot_class is the second half of train_hot_class (validation categorial classes)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaildation structure of pixels :  (6770, 100, 100, 3)\n",
      "Vaildation structure of categorial classes : (6770, 131)\n",
      "Train structure of images:  (60922, 100, 100, 3)\n",
      "Train structure of categorial classes :  (60922, 131)\n",
      "Test structure of images:  (22688, 100, 100, 3)\n",
      "Test structure of categorial classes :  (22688, 131)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images_array, valid_images_array, train_hot_class, valid_hot_class = train_test_split(train_images_array, train_hot_class, test_size = 0.1, random_state=42)\n",
    "\n",
    "print('Vaildation structure of pixels : ', valid_images_array.shape)\n",
    "print('Vaildation structure of categorial classes :', valid_hot_class.shape)\n",
    "print('Train structure of images: ', train_images_array.shape)\n",
    "print('Train structure of categorial classes : ', train_hot_class.shape)\n",
    "print('Test structure of images: ', test_images_array.shape)\n",
    "print('Test structure of categorial classes : ', test_hot_class.shape)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_datasize = 3600 #40000\n",
    "#names_valid, names_train = names_train[train_datasize:], names_train[:train_datasize]\n",
    "\n",
    "#valid_hot_class, train_hot_class = train_hot_class[train_datasize:], train_hot_class[:train_datasize]\n",
    "#print('Vaildation structure of names : ', names_valid.shape)\n",
    "#print('Vaildation structure of categorial classes :', valid_hot_class.shape)\n",
    "#print('Train structure of names: ', names_train.shape)\n",
    "#print('Train structure of categorial classes : ', train_hot_class.shape)\n",
    "#print('Test structure of names: ', names_test.shape)\n",
    "#print('Test structure of categorial classes : ', test_hot_class.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show 10 images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize =(30,5))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(train_images_array[i]))\n",
    "    print(names_train[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CNN architecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_10 (Flatten)        (None, 30000)             0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                300010    \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 300)               3300      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 300)               0         \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1000)              301000    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 1000)             4000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 1000)             4000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 100)               100100    \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 131)               13231     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,728,281\n",
      "Trainable params: 1,723,461\n",
      "Non-trainable params: 4,820\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D, AveragePooling2D, BatchNormalization\n",
    "import os\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(100,100,3)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(300)) #900\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1000)) #900\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1000)) #900\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100)) #900\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(131,activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled!\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['accuracy'])\n",
    "print('Compiled!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.41827, saving model to ann.hdf5\n",
      "1904/1904 - 26s - loss: 2.5950 - accuracy: 0.3046 - val_loss: 1.4183 - val_accuracy: 0.5168 - 26s/epoch - 14ms/step\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 2: val_loss improved from 1.41827 to 0.55451, saving model to ann.hdf5\n",
      "1904/1904 - 23s - loss: 1.3783 - accuracy: 0.5715 - val_loss: 0.5545 - val_accuracy: 0.8368 - 23s/epoch - 12ms/step\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 3: val_loss improved from 0.55451 to 0.48946, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 1.1087 - accuracy: 0.6520 - val_loss: 0.4895 - val_accuracy: 0.8510 - 22s/epoch - 11ms/step\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 4: val_loss improved from 0.48946 to 0.33889, saving model to ann.hdf5\n",
      "1904/1904 - 21s - loss: 0.9538 - accuracy: 0.6994 - val_loss: 0.3389 - val_accuracy: 0.9056 - 21s/epoch - 11ms/step\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.33889\n",
      "1904/1904 - 21s - loss: 0.8658 - accuracy: 0.7256 - val_loss: 0.7890 - val_accuracy: 0.7303 - 21s/epoch - 11ms/step\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.33889\n",
      "1904/1904 - 22s - loss: 0.8088 - accuracy: 0.7450 - val_loss: 0.8455 - val_accuracy: 0.7554 - 22s/epoch - 11ms/step\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 7: val_loss improved from 0.33889 to 0.26977, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 0.7473 - accuracy: 0.7640 - val_loss: 0.2698 - val_accuracy: 0.9168 - 22s/epoch - 11ms/step\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.26977\n",
      "1904/1904 - 21s - loss: 0.7052 - accuracy: 0.7775 - val_loss: 0.3175 - val_accuracy: 0.8959 - 21s/epoch - 11ms/step\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 9: val_loss improved from 0.26977 to 0.22665, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 0.6722 - accuracy: 0.7887 - val_loss: 0.2267 - val_accuracy: 0.9312 - 22s/epoch - 12ms/step\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 10: val_loss improved from 0.22665 to 0.17116, saving model to ann.hdf5\n",
      "1904/1904 - 21s - loss: 0.6456 - accuracy: 0.7966 - val_loss: 0.1712 - val_accuracy: 0.9480 - 21s/epoch - 11ms/step\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 11: val_loss improved from 0.17116 to 0.17017, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 0.6254 - accuracy: 0.8052 - val_loss: 0.1702 - val_accuracy: 0.9477 - 22s/epoch - 11ms/step\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.17017\n",
      "1904/1904 - 21s - loss: 0.5936 - accuracy: 0.8137 - val_loss: 0.2839 - val_accuracy: 0.9018 - 21s/epoch - 11ms/step\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 13: val_loss improved from 0.17017 to 0.13822, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 0.5714 - accuracy: 0.8219 - val_loss: 0.1382 - val_accuracy: 0.9582 - 22s/epoch - 11ms/step\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.13822\n",
      "1904/1904 - 22s - loss: 0.5580 - accuracy: 0.8251 - val_loss: 0.4959 - val_accuracy: 0.8394 - 22s/epoch - 11ms/step\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 15: val_loss improved from 0.13822 to 0.12859, saving model to ann.hdf5\n",
      "1904/1904 - 22s - loss: 0.5373 - accuracy: 0.8334 - val_loss: 0.1286 - val_accuracy: 0.9622 - 22s/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'ann.hdf5', verbose = 1, save_best_only = True)\n",
    "\n",
    "history = model.fit(train_images_array, train_hot_class,\n",
    "        batch_size = 32, #32\n",
    "        epochs = 15, #15\n",
    "        validation_data = (valid_images_array, valid_hot_class),\n",
    "        callbacks = [checkpointer],\n",
    "        verbose = 2, shuffle = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.8247091174125671\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('ann.hdf5')\n",
    "score = model.evaluate(test_images_array, test_hot_class, verbose = 0)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('/savedmodel/fullCNN')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(train_images_array[10].reshape(1,100,100,1))\n",
    " \n",
    "def display_activation(activations, col_size, row_size, act_index): \n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history.model.weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(test_images_array)\n",
    "\n",
    "fig = plt.figure(figsize=(32, 30))\n",
    "for i, idx in enumerate(np.random.choice(test_images_array.shape[0], size = 32, replace = False)):\n",
    "    ax = fig.add_subplot(8, 6, i + 1, xticks = [], yticks = [])\n",
    "    ax.imshow(np.squeeze(test_images_array[idx]))\n",
    "    pred_idx = np.argmax(y_pred[idx])\n",
    "    true_idx = np.argmax(test_hot_class[idx])\n",
    "    ax.set_title(\"{} ({})\".format(stringclass_test[pred_idx], stringclass_test[true_idx]),\n",
    "                 color = (\"green\" if pred_idx == true_idx else \"red\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 5))\n",
    "plt.plot(history.history['accuracy'], color = 'r')\n",
    "plt.plot(history.history['val_accuracy'], color = 'b')\n",
    "plt.title('Model Accuracy', weight = 'bold', fontsize = 16)\n",
    "plt.ylabel('accuracy', weight = 'bold', fontsize = 14)\n",
    "plt.xlabel('epoch', weight = 'bold', fontsize = 14)\n",
    "plt.ylim(0.4, 1)\n",
    "plt.xticks(weight = 'bold', fontsize = 12)\n",
    "plt.yticks(weight = 'bold', fontsize = 12)\n",
    "plt.legend(['train', 'val'], loc = 'lower right', prop = {'size': 14})\n",
    "plt.grid(color = 'y', linewidth = '0.5')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(1)  \n",
    "   \n",
    " # summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "   \n",
    " # summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}